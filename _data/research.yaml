categories:

  - data-filter: animation
    category-name: animation

  - data-filter: colorization
    category-name: colorization

  - data-filter: lane detection
    category-name: lane detection


projects:

  - title: 'Skinned Motion Retargeting with Preservation of Body Part Relationships'
    system-name: 
    gif: assets/img/mr/smr.png
    conference: IEEE Transactions on Visualization and Computer Graphics, TVCG 2024, CCF A
    conference-web: https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945
    status: 
    authors: <b><u>Jia-Qi Zhang</u></b>, Miao Wang, Fu-Cheng Zhang, Fang-Lue Zhang
    pdf: https://ieeexplore.ieee.org/document/10586814
    code: 
    demo: 
    slides: 
    talk: 
    abstract-less: Motion retargeting is an active research area in computer graphics and animation, allowing for the transfer of motion from one character to another, thereby creating diverse animated character data. While this technology has numerous applications in animation, games, and movies, current methods often produce unnatural or semantically inconsistent motion when applied
    abstract-more: to characters with different shapes or joint counts. This is primarily due to a lack of consideration for the geometric and spatial relationships between the body parts of the source and target characters. To tackle this challenge, we introduce a novel spatially-preserving Skinned Motion Retargeting Network (SMRNet) capable of handling motion retargeting for characters with varying shapes and skeletal structures while maintaining semantic consistency. By learning a hybrid representation of the character's skeleton and shape in a rest pose, SMRNet transfers the rotation and root joint position of the source character's motion to the target character through embedded rest pose feature alignment. Additionally, it incorporates a differentiable loss function to further preserve the spatial consistency of body parts between the source and target. Comprehensive quantitative and qualitative evaluations demonstrate the superiority of our approach over existing alternatives, particularly in preserving spatial relationships more effectively.
    tag: motion_retargeting
    category: animation

  - title: 'HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution'
    system-name: 
    gif: assets/img/lane/houghlanenet.png
    conference: Computers & Graphics, 2023, <strong>Best Paper</strong>, CCF C
    conference-web: https://www.sciencedirect.com/journal/computers-and-graphics
    status: 
    authors: <b><u>Jia-Qi Zhang</u></b>, Hao-Bin Duan, Jun-Long Chen, Ariel Shamir, Miao Wang
    pdf: https://arxiv.org/pdf/2307.03494.pdf
    code: https://github.com/Jiaqi-zhang/HoughLaneNet
    demo: 
    slides: 
    talk: 
    abstract-less: The task of lane detection has garnered considerable attention in the field of autonomous driving due to its complexity. Lanes can present difficulties for detection, as they can be narrow, fragmented, and often obscured by heavy traffic. However, it has been observed that the lanes have a geometrical structure that resembles a straight line, leading to improved lane detection 
    abstract-more: results when utilizing this characteristic. To address this challenge, we propose a hierarchical Deep Hough Transform (DHT) approach that combines all lane features in an image into the Hough parameter space. Additionally, we refine the point selection method and incorporate a Dynamic Convolution Module to effectively differentiate between lanes in the original image. Our network architecture comprises a backbone network, either a ResNet or Pyramid Vision Transformer, a Feature Pyramid Network as the neck to extract multi-scale features, and a hierarchical DHT-based feature aggregation head to accurately segment each lane. By utilizing the lane features in the Hough parameter space, the network learns dynamic convolution kernel parameters corresponding to each lane, allowing the Dynamic Convolution Module to effectively differentiate between lane features. Subsequently, the lane features are fed into the feature decoder, which predicts the final position of the lane. Our proposed network structure demonstrates improved performance in detecting heavily occluded or worn lane images, as evidenced by our extensive experimental results, which show that our method outperforms or is on par with state-of-the-art techniques.
    tag: lane_detection
    category: lane detection

  - title: Reference-Based Deep Line Art Video Colorization
    system-name: 
    gif: assets/img/videocolor/Colorization.png
    conference: IEEE Transactions on Visualization and Computer Graphics, TVCG 2022, CCF A
    conference-web: https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945
    status: 
    authors: Min Shi#, <b><u>Jia-Qi Zhang#</u></b>, Shu-Yu Chen, Lin Gao, Yu-Kun Lai, Fang-Lue Zhang
    pdf: https://ieeexplore.ieee.org/document/9693178
    code: 
    demo: https://youtu.be/-1e7hRmXCEY
    slides: 
    talk: 
    abstract-less: Coloring line art images based on the colors of reference images is a crucial stage in animation production, which is time-consuming and tedious. This paper proposes a deep architecture to automatically color line art videos with the same color style as the given reference images. Our framework consists of a color transform network and a temporal refinement 
    abstract-more: network based on 3U-net. The color transform network takes the target line art images as well as the line art and color images of the reference images as input and generates corresponding target color images. To cope with the large differences between each target line art image and the reference color images, we propose a distance attention layer that utilizes non-local similarity matching to determine the region correspondences between the target image and the reference images and transforms the local color information from the references to the target. To ensure global color style consistency, we further incorporate Adaptive Instance Normalization (AdaIN) with the transformation parameters obtained from a multiple-layer AdaIN that describes the global color style of the references extracted by an embedder network. The temporal refinement network learns spatiotemporal features through 3D convolutions to ensure the temporal color consistency of the results. Our model can achieve even better coloring results by fine-tuning the parameters with only a small number of samples when dealing with an animation of a new style. To evaluate our method, we build a line art coloring dataset. Experiments show that our method achieves the best performance on line art video coloring compared to the current state-of-the-art methods.
    tag: color_video
    category: colorization
  
  - title: 'A review of image and video colorization: From analogies to deep learning'
    system-name: 
    gif: assets/img/color_survey/survey.png
    conference: Visual Informatics, VI 2022
    conference-web: https://www.sciencedirect.com/journal/visual-informatics
    status: 
    authors: Shu-Yu Chen, <b><u>Jia-Qi Zhang</u></b>, You-You Zhao, Paul L. Rosin, Yu-Kun Lai, Lin Gao
    pdf: https://www.sciencedirect.com/science/article/pii/S2468502X22000389
    code: 
    demo: 
    slides: 
    talk: 
    abstract-less: Image colorization is a classic and important topic in computer graphics, where the aim is to add color to a monochromatic input image to produce a colorful result. In this survey, we present the history of colorization research in chronological order and summarize popular algorithms in this field. Early work on colorization mostly focused on developing techniques to improve the 
    abstract-more: colorization quality. In the last few years, researchers have considered more possibilities such as combining colorization with NLP (natural language processing) and focused more on industrial applications. To better control the color, various types of color control are designed, such as providing reference images or color-scribbles. We have created a taxonomy of the colorization methods according to the input type, divided into grayscale, sketch-based and hybrid. The pros and cons are discussed for each algorithm, and they are compared according to their main characteristics. Finally, we discuss how deep learning, and in particular Generative Adversarial Networks (GANs), has changed this field.
    tag: color_survey
    category: colorization

  - title: 'Write-An-Animation: High-level Text-based Animation Editing with Character-Scene Interaction'
    system-name: 
    gif: assets/img/text2anim/egteaser.png
    conference: Pacific Conference on Computer Graphics and Applications, PG 2021, CCF B
    conference-web: https://pg2022.org/
    status: 
    authors: <b><u>Jia-Qi Zhang</u></b>, Xiang Xu, Shen Z.M, Huang Z.H, Yang Zhao, Cao Y.P, Wan P.F, Miao Wang
    pdf: https://diglib.eg.org/bitstream/handle/10.1111/cgf14415/v40i7pp217-228.pdf
    code: 
    demo: https://youtu.be/3kgzasBL-UQ
    slides: 
    talk: 
    abstract-less: 3D animation production for storytelling requires essential manual processes of virtual scene composition, character creation, and motion editing, etc. Although professional artists can favorably create 3D animations using software, it remains a complex and challenging task for novice users to handle and learn such tools for content creation. In this paper, we present 
    abstract-more: Write-An-Animation, a 3D animation system that allows novice users to create, edit, preview, and render animations, all through text editing. Based on the input texts describing virtual scenes and human motions in natural languages, our system first parses the texts as semantic scene graphs, then retrieves 3D object models for virtual scene composition and motion clips for character animation. Character motion is synthesized with the combination of generative locomotions using neural state machine as well as template action motions retrieved from the dataset. Moreover, to make the virtual scene layout compatible with character motion, we propose an iterative scene layout and character motion optimization algorithm that jointly considers character-object collision and interaction. We demonstrate the effectiveness of our system with customized texts and public film scripts. Experimental results indicate that our system can generate satisfactory animations from texts.
    tag: anim_sence
    category: animation

  - title: Active Colorization for Cartoon Line Drawing
    system-name: 
    gif: assets/img/active/color2020.png
    conference: IEEE Transactions on Visualization and Computer Graphics, TVCG 2020, CCF A
    conference-web: https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945
    status: 
    authors: Shu-Yu Chen#, <b><u>Jia-Qi Zhang#</u></b>, Lin Gao, Yue He, Shi-Hong Xia, Min Shi, Fang-Lue Zhang
    pdf: https://ieeexplore.ieee.org/abstract/document/9143503
    code: 
    demo: https://youtu.be/OCkJhaoupcU
    slides: 
    talk: 
    abstract-less: In the animation industry, the colorization of raw sketch images is a vitally important but very time-consuming task. This article focuses on providing a novel solution that semiautomatically colorizes a set of images using a single colorized reference image. Our method is able to provide coherent colors for regions that have similar semantics to those in the reference image. An active learning
    abstract-more: based framework is used to match local regions, followed by mixed-integer quadratic programming (MIQP) which considers the spatial contexts to further refine the matching results. We efficiently utilize user interactions to achieve high accuracy in the final colorized images. Experiments show that our method outperforms the current state-of-the-art deep learning based colorization method in terms of color coherency with the reference image. The region matching framework could potentially be applied to other applications, such as color transfer.
    tag: color_sketch
    category: colorization



  # - title: Online 3D Deformable Object Classification for Mobile Cobot Manipulation
  #   system-name: 
  #   gif: assets/img/demo_online.gif
  #   conference: ISR Europe 2023 (Stuttgart, Baden-Wurttemberg, Germany)
  #   conference-web: https://www.isr-robotics.org/isr
  #   status: 
  #   authors: <u>Khang Nguyen</u>, Tuan Dang, Manfred Huber.
  #   pdf: https://www.researchgate.net/profile/Khang-Nguyen-133/publication/374371098_Online_3D_Deformable_Object_Classification_for_Mobile_Cobot_Manipulation/links/651a2cfa1e2386049def3947/Online-3D-Deformable-Object-Classification-for-Mobile-Cobot-Manipulation.pdf
  #   code: https://github.com/mkhangg/deformable_cobot
  #   demo: https://youtu.be/qkgi3T6xYzI
  #   slides: https://mkhangg.com/assets/slides/isr23_slides.pdf
  #   talk: https://youtu.be/ATzyXtLAK6E
  #   abstract-less: Vision-based object manipulation in assistive mobile cobots essentially relies on classifying the target objects based on their 3D shapes and features, whether they are deformed or not. In this work, we present an auto-generated dataset of deformed objects specific for assistive mobile cobot manipulation using an intuitive Laplacian-based mesh deformation procedure. We 
  #   abstract-more: first determine the graspable region of the robot hand on the given object's mesh. Then, we uniformly sample handle points within the graspable region and perform deformation with multiple handle points based on the robot gripper configuration. In each deformation, we identify the orientation of handle points and prevent self-intersection to guarantee the object's physical meaning when multiple handle points are simultaneously applied to the mesh at different deformation intensities. We also introduce a lightweight neural network for 3D deformable object classification. Finally, we test our generated dataset on the Baxter robot with two 7-DOF arms, an integrated RGB-D camera, and a 3D deformable object classifier. The result shows that the robot is able to classify real-world deformed objects from point clouds captured at multiple views by the RGB-D camera.
  #   tag: more_online
  #   category: perception manipulation
